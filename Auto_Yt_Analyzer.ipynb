{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto Yt Analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4PgiU84D_jz"
      },
      "source": [
        "# Master Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eEiSZIL4_i"
      },
      "source": [
        "def master(sd,ed,sp,wp,ip):\n",
        "  import json\n",
        "  from wordcloud import WordCloud, STOPWORDS \n",
        "  import matplotlib.pyplot as plt \n",
        "  import pandas as pd \n",
        "  import matplotlib.dates as mdates\n",
        "  from datetime import datetime, timedelta, time\n",
        "  from scipy.ndimage.filters import gaussian_filter1d\n",
        "  import traceback\n",
        "  from pathlib import Path, PureWindowsPath\n",
        "  from matplotlib.backends.backend_pdf import PdfPages\n",
        "  # Get this file from takeout.google.com\n",
        "  # To speed up the process, only select \"Google Chrome\"\n",
        "  # and within \"Google Chrome\" only select \"Browser History\".\n",
        "  # This file should be ready in a couple minutes\n",
        "  pp1=pp2=pp3=pp4=pp5=None\n",
        "  with open(Path(wp), encoding='utf8') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  temp=[]\n",
        "  for entry in data:\n",
        "    v=entry['time']\n",
        "    try:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    except:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%SZ')\n",
        "    if  v > ed :\n",
        "        continue\n",
        "    if (not (v >= sd and v <= ed )):\n",
        "        break\n",
        "    temp.append(entry)\n",
        "  data=temp\n",
        "\n",
        "  comment_words = '' \n",
        "  stopwords = set(STOPWORDS) \n",
        "  #print(type(stopwords))\n",
        "  stopwords.add('Watched')\n",
        "  stopwords.add('Visited')\n",
        "  # iterate through the csv file \n",
        "  for entry in data:\n",
        "      val = entry['title']\n",
        "      #print(val)\n",
        "      # typecaste each val to string \n",
        "      val = str(val)\n",
        "      # split the value \n",
        "      tokens = val.split() \n",
        "      # Converts each token into lowercase \n",
        "      for i in range(len(tokens)): \n",
        "          tokens[i] = tokens[i].lower() \n",
        "      \n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "  try:\n",
        "    wordcloud = WordCloud(width = 800, height = 800, \n",
        "                    background_color ='white', \n",
        "                    stopwords = stopwords, \n",
        "                    min_font_size = 10).generate(comment_words) \n",
        "  \n",
        "    # plot the WordCloud image                   \n",
        "    pp1=plt.figure(figsize = (8, 8), facecolor = None) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.tight_layout(pad = 0) \n",
        "    plt.show()\n",
        "  except:\n",
        "    print(\"Low Data Available\")\n",
        "    return\n",
        "  # Get this file from takeout.google.com\n",
        "  # To speed up the process, only select \"Google Chrome\"\n",
        "  # and within \"Google Chrome\" only select \"Browser History\".\n",
        "  # This file should be ready in a couple minutes\n",
        "  with open(Path(sp), encoding='utf8') as file:\n",
        "      data = json.load(file)\n",
        "  temp=[]\n",
        "  for entry in data:\n",
        "    v=entry['time']\n",
        "    try:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    except:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%SZ')\n",
        "    if  v > ed :\n",
        "        continue\n",
        "    if (not (v >= sd and v <= ed )):\n",
        "        break\n",
        "    temp.append(entry)\n",
        "  data=temp\n",
        "\n",
        "\n",
        "  comment_words = '' \n",
        "  stopwords = set(STOPWORDS) \n",
        "  #print(type(stopwords))\n",
        "  stopwords.add('Searched')\n",
        "  #stopwords.add('Visited')\n",
        "  # iterate through the csv file \n",
        "  for entry in data:\n",
        "      val = entry['title']\n",
        "      #print(val)\n",
        "      # typecaste each val to string \n",
        "      val = str(val)\n",
        "      # split the value \n",
        "      tokens = val.split() \n",
        "      # Converts each token into lowercase \n",
        "      for i in range(len(tokens)): \n",
        "          tokens[i] = tokens[i].lower() \n",
        "      \n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "  try:\n",
        "    wordcloud = WordCloud(width = 800, height = 800, \n",
        "                    background_color ='white', \n",
        "                    stopwords = stopwords, \n",
        "                    min_font_size = 10).generate(comment_words) \n",
        "\n",
        "    # plot the WordCloud image                   \n",
        "    pp2=plt.figure(figsize = (8, 8), facecolor = None) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.tight_layout(pad = 0) \n",
        "    plt.show() \n",
        "  except:\n",
        "    print(\"Low Data Available\")\n",
        "  with open(Path(sp), encoding='utf8') as file:\n",
        "    data1 = json.load(file)\n",
        "  with open(Path(wp), encoding='utf8') as file:\n",
        "      data2 = json.load(file)\n",
        "\n",
        "  temp=[]\n",
        "  for entry in data1:\n",
        "    v=entry['time']\n",
        "    try:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    except:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%SZ')\n",
        "    if  v > ed :\n",
        "        continue\n",
        "    if (not (v >= sd and v <= ed )):\n",
        "        break\n",
        "    temp.append(entry)\n",
        "  for entry in data2:\n",
        "    v=entry['time']\n",
        "    try:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    except:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%SZ')\n",
        "    if  v > ed :\n",
        "        continue\n",
        "    if (not (v >= sd and v <= ed )):\n",
        "        break\n",
        "    temp.append(entry)  \n",
        "  data=temp\n",
        "  times = []\n",
        "  # Histogram of when sites are loaded. \n",
        "  # This shows most/least active time of day\n",
        "  daily_minute = [0] * 1440\n",
        "  day_of_week = [0] * 7\n",
        "  # Number of visits for each day\n",
        "  daily_visits = {}\n",
        "  for entry in data: \n",
        "      val = entry['time']\n",
        "\n",
        "      #print(val)\n",
        "      # Average time of day\n",
        "      try:\n",
        "        val=datetime.strptime(val, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "      except:\n",
        "        val=datetime.strptime(val, '%Y-%m-%dT%H:%M:%SZ')\n",
        "      times.append(val)\n",
        "      time_idx = val.hour*60 + val.minute\n",
        "      daily_minute[time_idx] += 1\n",
        "\n",
        "      # Weekly usage\n",
        "      day_of_week[val.weekday()] += 1\n",
        "      \n",
        "      # Number of webpages per day\n",
        "      day_idx = val.strftime('%Y-%m-%d')\n",
        "      if day_idx in daily_visits.keys():\n",
        "          daily_visits[day_idx] += 1\n",
        "      else:\n",
        "          daily_visits[day_idx] = 1\n",
        "  \n",
        "  daily = dict(daily_visits)\n",
        "  sorted_visits = sorted(daily.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "  total = 0\n",
        "  for day in sorted_visits:\n",
        "      total += day[1]\n",
        "  ave = int(total / len(sorted_visits))\n",
        "  print('You Watched and Searched {}  in the last {} days!'.format(len(data), len(sorted_visits)))\n",
        "  print('That\\'s {} Watch&Search per day'.format(ave))\n",
        "  print()\n",
        "  print(\"Your most active days were:\")\n",
        "  try :\n",
        "    for i in range(5):\n",
        "        print(\"\\t{}) {} with {} Watch&Search\".format(str(i+1), sorted_visits[i][0], sorted_visits[i][1]))\n",
        "  except :\n",
        "    print()\n",
        "  if ip==3:\n",
        "        pp3=plt.figure()\n",
        "        start_day = datetime.strptime(list(daily.items())[-1][0], '%Y-%m-%d')\n",
        "        end_day = datetime.strptime(list(daily.items())[0][0], '%Y-%m-%d')\n",
        "        start_day=sd\n",
        "        end_day=ed\n",
        "        #print(start_day)\n",
        "        #print(end_day)\n",
        "        date_list = mdates.drange(start_day, end_day+timedelta(days=1), timedelta(days=1))\n",
        "        for date in date_list:\n",
        "            if not mdates.num2date(date).strftime('%Y-%m-%d') in daily.keys():\n",
        "                daily[mdates.num2date(date).strftime('%Y-%m-%d')] = 0\n",
        "\n",
        "        sorted_visits = sorted(daily.items(), key = lambda x:datetime.strptime(x[0], '%Y-%m-%d'), reverse=False)\n",
        "        #print(sorted_visits)\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%b '%y\"))\n",
        "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
        "        plt.gca().axes.set_ylabel('Number of Watch&Search Per Day')\n",
        "        plt.gca().axes.set_xlabel('Day')\n",
        "        plt.title('Watch&Search per Day')\n",
        "\n",
        "        visits = [x[1] for x in sorted_visits]\n",
        "        #print(visits)\n",
        "        visits_smoothed = gaussian_filter1d(visits, sigma=12)\n",
        "        #print(visits_smoothed)\n",
        "        plt.plot(date_list, visits_smoothed, '-', color='black')\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        #pp.savefig(pp3)\n",
        "    \n",
        "  else:\n",
        "        pp3=plt.figure()\n",
        "        start_day = datetime.strptime(list(daily.items())[-1][0], '%Y-%m-%d')\n",
        "        end_day = datetime.strptime(list(daily.items())[0][0], '%Y-%m-%d')\n",
        "        start_day=sd\n",
        "        end_day=ed\n",
        "        #print(start_day)\n",
        "        #print(end_day)\n",
        "        date_list = mdates.drange(start_day, end_day+timedelta(days=1), timedelta(days=1))\n",
        "        for date in date_list:\n",
        "            if not mdates.num2date(date).strftime('%Y-%m-%d') in daily.keys():\n",
        "                daily[mdates.num2date(date).strftime('%Y-%m-%d')] = 0\n",
        "\n",
        "        sorted_visits = sorted(daily.items(), key = lambda x:datetime.strptime(x[0], '%Y-%m-%d'), reverse=False)\n",
        "        #print(sorted_visits)\n",
        "        if ip==1:\n",
        "          plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%d %b\"))\n",
        "          plt.title('Watch&Search per Day')\n",
        "        elif ip==2:\n",
        "          plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%d\"))\n",
        "          #now = datetime.now()\n",
        "          plt.title(\"Watch&Search per Day in month of %s\" % (sd.strftime('%B %Y')))\n",
        "\n",
        "\n",
        "        plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
        "        plt.gca().axes.set_ylabel('Number of Watch&Search Per Day')\n",
        "        plt.gca().axes.set_xlabel('Day')\n",
        "        \n",
        "\n",
        "        visits = [x[1] for x in sorted_visits]\n",
        "        #print(visits)\n",
        "        #visits_smoothed = gaussian_filter1d(visits, sigma=12)\n",
        "        #print(visits_smoothed)\n",
        "        plt.plot(date_list, visits, '-', color='black')\n",
        "        plt.gcf().autofmt_xdate()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "        #pp.savefig(pp3)\n",
        "  \n",
        "  \n",
        "  pp4=plt.figure()\n",
        "  xlabs = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "  plt.xticks(range(7),xlabs)\n",
        "  plt.gca().axes.set_yticklabels([])\n",
        "  plt.gca().axes.set_ylabel('Average Weekly Usage')\n",
        "  plt.gca().axes.set_xlabel('Day of the Week')\n",
        "  plt.title('Average Youtube Usage per Week')\n",
        "  plt.bar(range(7), day_of_week, color='black');\n",
        "  # plt.gcf().autofmt_xdate()\n",
        "  # plt.grid()\n",
        "  plt.show()\n",
        "  max_index = day_of_week.index(max(day_of_week))\n",
        "  min_index = day_of_week.index(min(day_of_week))\n",
        "  print('It looks like you\\'re most active on {} and least active on {}'\n",
        "        .format(xlabs[max_index], xlabs[min_index]))  \n",
        "  pp5=plt.figure()\n",
        "  start_time = datetime(12, 12, 12, 0, 0)\n",
        "  #print(start_time)\n",
        "  end_time = start_time + timedelta(hours=24)\n",
        "  time_list = mdates.drange(start_time, end_time, timedelta(minutes=1))\n",
        "  hour_list = mdates.drange(start_time, end_time+timedelta(hours=1), timedelta(hours=2))\n",
        "  if ip==3 :\n",
        "    daily_minute_smoothed = gaussian_filter1d(daily_minute, sigma=14)\n",
        "  else :\n",
        "    daily_minute_smoothed = daily_minute\n",
        "  plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%I %p'))\n",
        "  plt.xticks(hour_list)\n",
        "  plt.gca().axes.set_yticklabels([])\n",
        "  plt.gca().axes.set_ylabel('Average Number of Watch&Search')\n",
        "  plt.gca().axes.set_xlabel('Time of Day (Minutes)')\n",
        "  plt.title('Average Youtube Usage per Day')\n",
        "  plt.plot(time_list, daily_minute_smoothed, '-', color='black');\n",
        "  plt.gcf().autofmt_xdate()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  max_index = list(daily_minute_smoothed).index(max(daily_minute_smoothed))\n",
        "  min_index = list(daily_minute_smoothed).index(min(daily_minute_smoothed))\n",
        "  most_active = (mdates.num2date(time_list[max_index]).strftime('%I:%m %p'))\n",
        "  least_active = (mdates.num2date(time_list[min_index]).strftime('%I:%m %p'))\n",
        "  print('It looks like you\\'re most active around {} and least active around {}'\n",
        "        .format(most_active, least_active))\n",
        "  most_active = '{:02d}:{:02d}'.format(*divmod(max_index, 60))\n",
        "  least_active = '{:02d}:{:02d}'.format(*divmod(min_index, 60))\n",
        "  print('Can be Accurately It looks like you\\'re most active around {} and least active around {}'\n",
        "        .format(most_active, least_active))\n",
        "  # if (pp1 is not None) and (pp2 is not None):\n",
        "  #   pp = PdfPages(\"drive/My Drive/temp/%s-%s.pdf\"%(sd.strftime(\"%d-%m-%Y\"),ed.strftime(\"%d-%m-%Y\")))\n",
        "  #   pp.savefig(pp1)\n",
        "  #   pp.savefig(pp2)\n",
        "  #   pp.savefig(pp3)\n",
        "  #   pp.savefig(pp4)\n",
        "  #   pp.savefig(pp5)\n",
        "  #   pp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ6TujEvEFH2"
      },
      "source": [
        "# Current & Previous Month Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W9n06KHL8ao"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta, time \n",
        "import time\n",
        "import calendar\n",
        "import traceback\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "def Remove(duplicate): \n",
        "    final_list = [] \n",
        "    for num in duplicate: \n",
        "        if num not in final_list: \n",
        "            final_list.append(num) \n",
        "    return final_list \n",
        "\n",
        "arr = os.listdir('drive/My Drive/Takeout')\n",
        "#print(arr)\n",
        "for val in arr:\n",
        "  path_to_zip_file='drive/My Drive/Takeout/'+str(val)\n",
        "  modificationTime = time.strftime('%d-%m-%Y', time.localtime(os.path.getmtime(path_to_zip_file)))\n",
        "  #print(\"Last Modified Time : \", modificationTime )\n",
        "  present = datetime.now()\n",
        "  present = present.strftime('%d-%m-%Y')\n",
        "  if modificationTime != present:\n",
        "      continue \n",
        "  #print(modificationTime)\n",
        "  directory_to_extract_to='drive/My Drive/Extract_Takeout/'\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "  search_path=find('search-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  watch_path=find('watch-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  if(search_path is None):\n",
        "    print(\"No File Found!!!\")\n",
        "    continue\n",
        "  day = modificationTime\n",
        "  dt = datetime.strptime(day, '%d-%m-%Y')\n",
        "  cld=calendar.Calendar(firstweekday=0)\n",
        "  sds=[]\n",
        "  eds=[]\n",
        "  cm=[str(dt.month),str(dt.year)]\n",
        "  temp=dt\n",
        "  first =temp.replace(day=1)\n",
        "  lastMonth = first - timedelta(days=1)\n",
        "  pm=[lastMonth.strftime(\"%m\"),lastMonth.strftime(\"%Y\")]\n",
        "  cpm=[pm,cm]\n",
        "  for val in cpm:\n",
        "    y=int(val[1])\n",
        "    m=int(val[0]) \n",
        "    for end_day in cld.itermonthdates(y,m):\n",
        "      if end_day.weekday()==5:\n",
        "        start_day=end_day-timedelta(6)\n",
        "        sds.append(start_day.isoformat())\n",
        "        eds.append(end_day.isoformat())\n",
        "        #print('{} - {}'.format(start_day.isoformat(),end_day.isoformat()))\n",
        "  sds = Remove(sds)\n",
        "  eds = Remove(eds)\n",
        "  print()\n",
        "  #print(sds)\n",
        "  #print(eds)\n",
        "  #t=0\n",
        "  for i,j in zip(sds,eds):\n",
        "    try:\n",
        "      #print(search_path)\n",
        "      #print(watch_path)\n",
        "      i = datetime.strptime(i, '%Y-%m-%d')\n",
        "      j = datetime.strptime(j, '%Y-%m-%d')\n",
        "      master(i,j,search_path,watch_path,1)\n",
        "      #t+=1\n",
        "    except Exception as e: \n",
        "        traceback.print_exc()\n",
        "        #print(\"error\")\n",
        "  #print(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-tjjrV8EJlP"
      },
      "source": [
        "# Specific Year all Month Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nBB_3vRythE0"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta, time \n",
        "import time\n",
        "import calendar\n",
        "import traceback\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "def Remove(duplicate): \n",
        "    final_list = [] \n",
        "    for num in duplicate: \n",
        "        if num not in final_list: \n",
        "            final_list.append(num) \n",
        "    return final_list \n",
        "\n",
        "arr = os.listdir('drive/My Drive/Takeout')\n",
        "#print(arr)\n",
        "for val in arr:\n",
        "  path_to_zip_file='drive/My Drive/Takeout/'+str(val)\n",
        "  modificationTime = time.strftime('%d-%m-%Y', time.localtime(os.path.getmtime(path_to_zip_file)))\n",
        "  #print(\"Last Modified Time : \", modificationTime )\n",
        "  present = datetime.now()\n",
        "  present = present.strftime('%d-%m-%Y')\n",
        "  if modificationTime != present:\n",
        "      continue \n",
        "  #print(modificationTime)\n",
        "  directory_to_extract_to='drive/My Drive/Extract_Takeout/'\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "  search_path=find('search-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  watch_path=find('watch-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  if(search_path is None):\n",
        "    print(\"No File Found!!!\")\n",
        "    continue\n",
        "  e_year = input(\"Enter Year:\")\n",
        "  e_year=int(e_year)\n",
        "  for m in range(1,12):\n",
        "    import datetime\n",
        "    _, num_days = calendar.monthrange(e_year, m)\n",
        "    first_day = datetime.date(e_year, m, 1)\n",
        "    last_day = datetime.date(e_year, m, num_days)\n",
        "    first_day=first_day.strftime('%Y-%m-%d')\n",
        "    last_day=last_day.strftime('%Y-%m-%d')\n",
        "    try:\n",
        "      #print(search_path)\n",
        "      #print(watch_path)\n",
        "      i = datetime.datetime.strptime(first_day, '%Y-%m-%d')\n",
        "      j = datetime.datetime.strptime(last_day, '%Y-%m-%d')\n",
        "      master(i,j,search_path,watch_path,2)\n",
        "      #t+=1\n",
        "    except Exception as e: \n",
        "        traceback.print_exc()\n",
        "        #print(\"error\")\n",
        "  #print(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1RXepREkG3"
      },
      "source": [
        "# All History Year Wise Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CFHWYdcEYsG"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta, time \n",
        "import time\n",
        "import calendar\n",
        "import traceback\n",
        "from pathlib import Path, PureWindowsPath\n",
        "def find(name, path):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if name in files:\n",
        "            return os.path.join(root, name)\n",
        "def Remove(duplicate): \n",
        "    final_list = [] \n",
        "    for num in duplicate: \n",
        "        if num not in final_list: \n",
        "            final_list.append(num) \n",
        "    return final_list \n",
        "\n",
        "arr = os.listdir('drive/My Drive/Takeout')\n",
        "#print(arr)\n",
        "for val in arr:\n",
        "  path_to_zip_file='drive/My Drive/Takeout/'+str(val)\n",
        "  modificationTime = time.strftime('%d-%m-%Y', time.localtime(os.path.getmtime(path_to_zip_file)))\n",
        "  #print(\"Last Modified Time : \", modificationTime )\n",
        "  present = datetime.now()\n",
        "  present = present.strftime('%d-%m-%Y')\n",
        "  if modificationTime != present:\n",
        "      continue \n",
        "  #print(modificationTime)\n",
        "  directory_to_extract_to='drive/My Drive/Extract_Takeout/'\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "  search_path=find('search-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  watch_path=find('watch-history.json','drive/My Drive/Extract_Takeout/')\n",
        "  if(search_path is None):\n",
        "    print(\"No File Found!!!\")\n",
        "    continue\n",
        "\n",
        "  temp=set()\n",
        "  with open(Path(watch_path), encoding='utf8') as file:\n",
        "      data = json.load(file)\n",
        "  for entry in data:\n",
        "    v=entry['time']\n",
        "    try:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        temp.add(v.year)\n",
        "    except:\n",
        "        v=datetime.strptime(v, '%Y-%m-%dT%H:%M:%SZ')\n",
        "        temp.add(v.year)\n",
        "    \n",
        "    \n",
        "  #print(temp)\n",
        "  for e_year in temp:\n",
        "    e_year=int(e_year)\n",
        "    i=datetime(e_year,1,1)\n",
        "    j=datetime(e_year,12,31)    \n",
        "    try:\n",
        "      #print(search_path)\n",
        "      #print(watch_path)\n",
        "      #i = datetime.strptime(i, '%Y-%m-%d')\n",
        "      #j = datetime.strptime(j, '%Y-%m-%d')\n",
        "      master(i,j,search_path,watch_path,3)\n",
        "      #t+=1\n",
        "    except Exception as e: \n",
        "        traceback.print_exc()\n",
        "        #print(\"error\")\n",
        "  #print(t)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}